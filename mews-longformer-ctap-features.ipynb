{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "from transformers import LongformerTokenizerFast, LongformerForSequenceClassification, Trainer, TrainingArguments, LongformerConfig, AutoConfig, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, recall_score, confusion_matrix, cohen_kappa_score\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "pd.options.mode.chained_assignment = None"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-08T18:00:22.691088Z",
     "iopub.execute_input": "2023-05-08T18:00:22.691418Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# set parameters\n",
    "class Parameters:\n",
    "    Longformer = 'allenai/longformer-large-4096'\n",
    "    MAX_LEN = 1024\n",
    "    BATCH_SIZE = 1\n",
    "    EPOCHS = 1\n",
    "    LEARNING_RATE = 1e-5\n",
    "    RANDOM_SEED = 42\n",
    "    FOLDS = 10\n",
    "    TARGET_LIST = [0,1,2,3,4,5,6]"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Step 1: Pre-processing of Data and CTAP Features**\n",
    "\n",
    "The file Labels_texts_CTAPfeatures.xlsx is extracted into two dataframes (prompt AD and TE) with the following columns:\n",
    " \n",
    "id\ttext\tSpr_fs_facets_rounded\tStr_fs_facets_rounded\tInh_fs_facets_rounded\tctap\n",
    " \n",
    "Spr_fs_facets_rounded,Str_fs_facets_rounded,Inh_fs_facets_rounded are the three traits, that we try to learn and predict. Each traits contains 7 classes.\n",
    "\n",
    "ctap contains all the CTAP Features in arrays."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def detect_multicollinearity(df, threshold):\n",
    "    \"\"\"\n",
    "    Detects multicollinearity in a DataFrame and returns a list of non-multicollinear variables.\n",
    "    :param df: DataFrame\n",
    "    :param threshold: correlation threshold above which to detect multicollinearity\n",
    "    :return: list of non-multicollinear variable names\n",
    "    \"\"\"\n",
    "    colnames=list(df.columns)\n",
    "    var_list=[]\n",
    "    cols=colnames[:-1]\n",
    "    \n",
    "    for n in cols:\n",
    "        colnames.remove(n)\n",
    "        for i in colnames:\n",
    "            if i == n:\n",
    "                pass\n",
    "            else:\n",
    "                c1=df[[n,i]].corr().min()[0]\n",
    "                if c1 >= threshold:\n",
    "                    var_list.append(i)\n",
    "                else:\n",
    "                    pass\n",
    "    \n",
    "    colnames=list(df.columns)\n",
    "    cols=[]\n",
    "    \n",
    "    for c in colnames:\n",
    "        if c in var_list:\n",
    "            pass\n",
    "        else:\n",
    "            cols.append(c)\n",
    "                \n",
    "    return(cols)\n",
    "\n",
    "def detect_na_columns(df):\n",
    "    \"\"\"\n",
    "    Detects columns containing missing values (NAs) in a DataFrame and returns a list of column names.\n",
    "    :param df: DataFrame\n",
    "    :return: list of column names containing NAs\n",
    "    \"\"\"\n",
    "    na_cols = df.isna().any()\n",
    "    na_cols = na_cols[na_cols].index.tolist()\n",
    "    return na_cols\n",
    "\n",
    "def get_data_with_ctap(xlsx_file, prompt):\n",
    "    df = pd.read_excel(xlsx_file)\n",
    "    prompt_df = df[df[\"task\"]==prompt].reset_index()\n",
    "    \n",
    "    # QUESTION: what are the columns with NA values?\n",
    "    na_cols = detect_na_columns(prompt_df)\n",
    "    prompt_df = prompt_df.drop(columns=na_cols)\n",
    "    \n",
    "    data = prompt_df[['id','text','Spr_fs_facets_rounded', 'Str_fs_facets_rounded', 'Inh_fs_facets_rounded']]\n",
    "    \n",
    "    # QUESTION: does removing multicollinearity make a difference?\n",
    "    # features = detect_multicollinearity(prompt_df[prompt_df.columns[22:]], threshold=0.90).to_numpy()\n",
    "    \n",
    "    features = prompt_df[prompt_df.columns[22:]].to_numpy()\n",
    "    data['ctap'] = features.tolist()\n",
    "    \n",
    "    \n",
    "    return data\n",
    "    "
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_AD = get_data_with_ctap('/kaggle/input/mews-essays/Labels_texts_CTAPfeatures.xlsx', 'AD')\n",
    "num_features_AD = len(df_AD['ctap'].iloc[0])\n",
    "df_TE = get_data_with_ctap('/kaggle/input/mews-essays/Labels_texts_CTAPfeatures.xlsx', 'TE')\n",
    "num_features_TE = len(df_TE['ctap'].iloc[0])\n",
    "\n",
    "print(\"Number of CTAP Features: AD-\"+str(num_features_AD)+\", TE-\"+str(num_features_TE))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**STEP 2: Split of Datasets**\n",
    "\n",
    "For each dataframe, we first split 10% data as validation set. Then split the rest data into 5 Folds.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def get_k_folds(k, random_state, id_set):\n",
    "    k_folds_train = []\n",
    "    k_folds_test = []\n",
    "    kf = KFold(n_splits=k, random_state=random_state, shuffle=True)\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(id_set)):\n",
    "        # print(f\"Fold {i}:\")\n",
    "        # print(f\"  Amount Train: {len(train_index)}\")\n",
    "        # print(f\"  Amount Test:  {len(test_index)}\")\n",
    "        k_folds_train.append([id_set[x] for x in train_index])\n",
    "        k_folds_test.append([id_set[x] for x in test_index])\n",
    "    return k_folds_train, k_folds_test"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def validation_train_test_split(df):\n",
    "    validate_ids = random.sample(list(df['id'].unique()),int(len(df['id'].unique()) / 10))\n",
    "    validate = df[df['id'].isin(validate_ids)]\n",
    "    rest_ids = [item for item in list(df['id'].unique()) if item not in validate_ids]\n",
    "    train_ids, test_ids = get_k_folds(Parameters.FOLDS, Parameters.RANDOM_SEED, rest_ids)\n",
    "    train = []\n",
    "    test = []\n",
    "    for i in range(Parameters.FOLDS):\n",
    "        train.append(df[df['id'].isin(train_ids[i])])\n",
    "        # print(len(train[i]))\n",
    "        test.append(df[df['id'].isin(test_ids[i])])\n",
    "        # print(len(test[i]))\n",
    "\n",
    "    return validate, train, test\n",
    "\n",
    "validate_AD, train_AD, test_AD = validation_train_test_split(df_AD)\n",
    "validate_TE, train_TE, test_TE = validation_train_test_split(df_TE)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Step 3: Datasets and Model**\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class MEWSDataset(Dataset):\n",
    "    def __init__(self, data, max_len, tokenizer, target, extra_feature=None):\n",
    "        self.data = data\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = data['text'].values\n",
    "        self.targets = data[Parameters.TARGET_LIST].values\n",
    "        self.essay_id = data['id'].values\n",
    "        if extra_feature!=None:\n",
    "            self.extra_feature = data[extra_feature].values\n",
    "        else:\n",
    "            self.extra_feature = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.tokenizer.encode_plus(self.text[index].lower(),\n",
    "                                            truncation=True,\n",
    "                                            padding='max_length',\n",
    "                                            add_special_tokens=True,\n",
    "                                            return_attention_mask=True,\n",
    "                                            return_token_type_ids=True,\n",
    "                                            max_length=self.max_len,\n",
    "                                            return_tensors='pt')\n",
    "\n",
    "        input_ids = inputs['input_ids'].flatten()\n",
    "        attention_mask = inputs['attention_mask'].flatten()\n",
    "        token_type_ids = inputs['token_type_ids'].flatten()\n",
    "        targets = torch.FloatTensor(self.targets[index])\n",
    "        #print(targets)\n",
    "\n",
    "        if len(self.extra_feature)>0:\n",
    "            return {'input_ids': input_ids, 'attention_mask': attention_mask,'extra_data': torch.FloatTensor(self.extra_feature[index]), 'token_type_ids': token_type_ids,'targets': targets}\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids,'targets': targets}"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class LongformerWithCustomFeatureModel(torch.nn.Module):\n",
    "    def __init__(self, model_name, num_extra_dims, num_labels):\n",
    "        super().__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.transformer = AutoModel.from_pretrained(model_name, config=self.config)\n",
    "        num_hidden_size = self.transformer.config.hidden_size\n",
    "        self.classifier = torch.nn.Linear(num_hidden_size + num_extra_dims, num_labels)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.linear = torch.nn.Linear(768, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, extra_data, attention_mask=None):\n",
    "        hidden_states = self.transformer(input_ids=input_ids,attention_mask=attention_mask)  # [batch size, sequence length, hidden size]\n",
    "        cls_embeds = hidden_states.last_hidden_state[:, 0, :]  # [batch size, hidden size]\n",
    "        concat = torch.cat((cls_embeds, extra_data), dim=-1)  # [batch size, hidden size+num extra dims]\n",
    "        output = self.classifier(concat)  # [batch size, num labels]\n",
    "        return output\n",
    "\n",
    "    \n",
    "class LongformerModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LongformerModel, self).__init__()\n",
    "        self.model = LongformerForSequenceClassification.from_pretrained(Parameters.Longformer, return_dict=True, num_labels=len(Parameters.TARGET_LIST))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output = self.model(input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids).logits\n",
    "        \n",
    "        #output_label = output.logits.argmax().item()\n",
    "\n",
    "        return output\n",
    "\n",
    "    \n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\n",
    "\n",
    "def get_optimizer(model, learning_rate):\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    return optimizer"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def build_model_tokenizer(withCustomFeature, num_extra_dims, model_path=None):\n",
    "    # Tokenizer\n",
    "    tokenizer = LongformerTokenizerFast.from_pretrained(Parameters.Longformer)\n",
    "\n",
    "    # Modell\n",
    "    if withCustomFeature:\n",
    "        model = LongformerWithCustomFeatureModel(Parameters.Longformer, num_extra_dims, len(Parameters.TARGET_LIST))\n",
    "    else:\n",
    "        model = LongformerModel()\n",
    "    \n",
    "    if model_path is not None:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    return tokenizer, model"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_model(n_epochs,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                test_loader,\n",
    "                model, lr,\n",
    "                device, extra_data=None):\n",
    "    optimizer = get_optimizer(model, lr)\n",
    "    model.to(device)\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        model.train()\n",
    "        print(f' Epoch: {epoch + 1} - Train Set '.center(50, '='))\n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader)):\n",
    "            input_ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "            attention_mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = batch['targets'].to(device, dtype=torch.float)\n",
    "            \n",
    "            if extra_data is None:\n",
    "                outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            else:\n",
    "                extra_data = batch[extra_data].to(device, dtype=torch.long)\n",
    "                outputs = model(input_ids, extra_data, attention_mask)\n",
    "            # print(outputs)\n",
    "            # print(targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
    "            del input_ids, attention_mask, token_type_ids, targets, outputs\n",
    "            gc.collect()\n",
    "\n",
    "        print(f' Epoch: {epoch + 1} - Validation Set '.center(50, '='))\n",
    "        model.eval()\n",
    "        val_targets = []\n",
    "        val_outputs = []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(tqdm(val_loader)):\n",
    "                input_ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "                attention_mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "                token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "                targets = data['targets'].to(device, dtype=torch.float)\n",
    "                if extra_data is None:\n",
    "                    outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "                else:\n",
    "                    extra_data = data[extra_data].to(device, dtype=torch.long)\n",
    "                    outputs = model(input_ids, extra_data, attention_mask)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                val_loss = val_loss + ((1 / (batch_idx + 1)) * (loss.item() - val_loss))\n",
    "                val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "                val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "                del input_ids, attention_mask, token_type_ids, targets, outputs\n",
    "                gc.collect()\n",
    "            train_loss = train_loss / len(train_loader)\n",
    "            val_loss = val_loss / len(val_loader)\n",
    "            print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f} \\n'.format(\n",
    "                epoch + 1,\n",
    "                train_loss,\n",
    "                val_loss\n",
    "            ))\n",
    "        val_outputs_labels = np.array([np.argmax(a) for a in val_outputs])\n",
    "        val_targets_labels = np.array([np.argmax(a) for a in val_targets])\n",
    "        val_qwk = cohen_kappa_score(val_targets_labels, val_outputs_labels, weights='quadratic')\n",
    "        print(f\"Validation QWK: {round(val_qwk, 4)}\")\n",
    "\n",
    "        print('Test')\n",
    "        model.eval()\n",
    "        test_targets = []\n",
    "        test_outputs = []\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(tqdm(test_loader)):\n",
    "                input_ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "                attention_mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "                token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "                targets = data['targets'].to(device, dtype=torch.float)\n",
    "                if extra_data is None:\n",
    "                    outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "                else:\n",
    "                    extra_data = data[extra_data].to(device, dtype=torch.long)\n",
    "                    outputs = model(input_ids, extra_data, attention_mask)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.item() - test_loss))\n",
    "                test_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "                test_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "        test_outputs_labels = np.array([np.argmax(a) for a in test_outputs])\n",
    "        test_targets_labels = np.array([np.argmax(a) for a in test_targets])\n",
    "        accuracy = accuracy_score(test_targets_labels, test_outputs_labels)\n",
    "        recall_micro = recall_score(test_targets_labels, test_outputs_labels, average='micro')\n",
    "        recall_macro = recall_score(test_targets_labels, test_outputs_labels, average='macro')\n",
    "        f1_score_micro = f1_score(test_targets_labels, test_outputs_labels, average='micro')\n",
    "        f1_score_macro = f1_score(test_targets_labels, test_outputs_labels, average='macro')\n",
    "        qwk = cohen_kappa_score(test_targets_labels, test_outputs_labels, weights='quadratic')\n",
    "        print(f\"Test Loss: {round(test_loss, 4)}\")\n",
    "        print(f\"Accuracy Score: {round(accuracy, 4)}\")\n",
    "        print(f\"Recall (Micro): {round(recall_micro, 4)}\")\n",
    "        print(f\"Recall (Macro): {round(recall_macro, 4)}\")\n",
    "        print(f\"F1 Score (Micro): {round(f1_score_micro, 4)}\")\n",
    "        print(f\"F1 Score (Macro): {round(f1_score_macro, 4)} \\n\")\n",
    "        print(f\"QWK: {round(qwk, 4)}\")\n",
    "        cm = confusion_matrix(test_targets_labels, test_outputs_labels)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**STEP 4: Training Pipeline**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def transfer_targets(df, target):\n",
    "    for col in Parameters.TARGET_LIST:\n",
    "        df[col] = np.where(df[target] == col, 1, 0)\n",
    "    return df"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Pipeline 1 - Baseline Longformer without extra features\n",
    "#tokenizer, model = build_model_tokenizer(withCustomFeature=False, num_extra_dims=0)\n",
    "\n",
    "\n",
    "#valid_dataset = MEWSDataset(transfer_targets(validate_AD,'Spr_fs_facets_rounded'), max_len=Parameters.MAX_LEN,tokenizer=tokenizer,target='Spr_fs_facets_rounded')\n",
    "#val_data_loader = DataLoader(valid_dataset,shuffle=False,batch_size=Parameters.BATCH_SIZE)\n",
    "\n",
    "# TODO: Run 10 Folds, 10 Epochs\n",
    "#for i in range(Parameters.FOLDS):\n",
    "#train_dataset = MEWSDataset(transfer_targets(train_AD[i],'Spr_fs_facets_rounded'), max_len=Parameters.MAX_LEN,tokenizer=tokenizer,target='Spr_fs_facets_rounded')\n",
    "#train_data_loader = DataLoader(train_dataset,shuffle=True,batch_size=Parameters.BATCH_SIZE)\n",
    "#test_dataset = MEWSDataset(transfer_targets(test_AD[i],'Spr_fs_facets_rounded'), max_len=Parameters.MAX_LEN,tokenizer=tokenizer,target='Spr_fs_facets_rounded')\n",
    "#test_data_loader = DataLoader(test_dataset,shuffle=False,batch_size=Parameters.BATCH_SIZE)\n",
    "#model = train_model(n_epochs=Parameters.EPOCHS, train_loader=train_data_loader, val_loader=val_data_loader,test_loader=test_data_loader,model=model, lr=Parameters.LEARNING_RATE, device=device)\n",
    "    \n",
    "    # TODO: Run three targets in ['Spr_fs_facets_rounded', 'Str_fs_facets_rounded', 'Inh_fs_facets_rounded']\n",
    "    # TODO: Run two prompts in ['AD', 'TE']"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Pipeline 2 - Longformer with extra feature\n",
    "#tokenizer, model = build_model_tokenizer(withCustomFeature=True, num_extra_dims=num_features_AD)\n",
    "\n",
    "#valid_dataset = MEWSDataset(validate_AD, max_len=Parameters.MAX_LEN,tokenizer=tokenizer,target='Spr_fs_facets_rounded', extra_feature='ctap')\n",
    "#val_data_loader = DataLoader(valid_dataset,shuffle=False,batch_size=Parameters.BATCH_SIZE)\n",
    "#for i in range(Parameters.FOLDS):\n",
    "#    train_dataset = MEWSDataset(train_AD[i], max_len=Parameters.MAX_LEN,tokenizer=tokenizer,target='Spr_fs_facets_rounded', extra_feature='ctap')\n",
    "#    train_data_loader = DataLoader(train_dataset,shuffle=True,batch_size=Parameters.BATCH_SIZE)\n",
    "#    test_dataset = MEWSDataset(test_AD[i], max_len=Parameters.MAX_LEN,tokenizer=tokenizer,target='Spr_fs_facets_rounded', extra_feature='ctap')\n",
    "#    test_data_loader = DataLoader(test_dataset,shuffle=False,batch_size=Parameters.BATCH_SIZE)\n",
    "#    model = train_model(n_epochs=Parameters.EPOCHS, train_loader=train_data_loader, val_loader=val_data_loader,test_loader=test_data_loader,model=model, targets='Spr_fs_facets_rounded', lr=Parameters.LEARNING_RATE, extra_data='ctap', device=device)\n",
    "    # TODO: Run two prompts in ['AD', 'TE']\n",
    "    # TODO: Run three targets in ['Spr_fs_facets_rounded', 'Str_fs_facets_rounded', 'Inh_fs_facets_rounded']\n",
    "     \n",
    "    \n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
